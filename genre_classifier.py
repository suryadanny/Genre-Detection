# -*- coding: utf-8 -*-
"""Genre Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ulGDL-gMEY0dqwkUFNzZsdIS9WwN8GS
"""

import json, os

import numpy as np
import tensorflow as tf

from keras.preprocessing import sequence
from keras.models import Sequential, model_from_json
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.embeddings import Embedding
from keras.utils.np_utils import to_categorical
from keras.layers.recurrent import LSTM
from keras.layers.wrappers import Bidirectional
from keras import optimizers


from keras.layers import TextVectorization
from keras.preprocessing.text import Tokenizer
from keras import layers

import pandas as pd

from nltk.corpus import stopwords
from sklearn.metrics import hamming_loss
from sklearn.metrics import accuracy_score
from sklearn.externals import joblib


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer



seed = 21

np.random.seed(seed)

df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
                   columns=['a', 'b', 'c'])

frame = df.drop(['a','b'],axis = 1).values
frame = frame.reshape(frame.shape[0],).tolist()
print(df)
print(frame)
print(type(frame))

df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
                   columns=['a', 'b', 'c'])

frame = df.drop(['a','b'],axis = 1).values.tolist()
print(frame)
print(len(frame))
print(df.drop(['a','b'],axis = 1).values.reshape(3,).tolist())
print(len(df.drop(['a','b'],axis = 1).values.reshape(3,).tolist()))

def predict():
        plots = request.POST['plots']

        if not os.path.exist('model'):
               os.mknod('model')

               print("Loading processed training data")
               dataframe = pd.read_csv('movies_genres.csv',delimiter='\t')

               genres = dataframe.drop(['title','plot','plot_lang'],axis = 1).values
               genres = genres.reshape(genres.shape[0],).tolist()

               xlist = data_x = dataframe[['plot']].values.tolist()
               ylist = genres



               for j in range(len(xlist)):
                 wordList = xlist[j].split()
                 filtered_list = [ word for word in wordList if word not in stopwords.words('english')]
                 xlist[j] = " ".join(filtered_list)

               x = np.array(xlist)
               y = np.array(ylist)
               max_len = 500
               

               textVectorizer = TextVectorization( max_tokens=None, 
                                                  output_mode='int',
                                                  standardize='lower_and_strip_punctuation',
                                                  split='whitespace',
                                                  output_sequence_length=max_len)
               
               textVectorizer.adapt(x)

               indexed_words = { value: k+2 for k, value in enumerate(textVectorizer.get_vocabulary()[2:])}
               print('Indentified %s unique tokens' % len(indexed_words))
               x_train = textVectorizer(x).numpy()


               labeller =  MultiLabelBinarizer()

               y_train = labeller.fit_transform(y)


               #
               #  setting the model parameters

               epochs = 100
               batch_size = 64


               max_features = len(indexed_words)+2

               x_train , x_test , y_train, y_test = train_test_split(x_train,y_train , test_size = 0.25 , random_state = seed)
               label_len = len(y_train[0])

               # Forming the Embedding layer and Matrix

               embed_dict = {}
               read = open('glove.6B.100d.txt')
               for line in read:
                    values = line.split()
                    word = values[0]
                    coeff = np.asarray(values[1:],dtype = 'float32')
                    embed_dict[word] = coeff
               read.close()

               embed_matrix = np.zeros((max_features,100))

               for word, i in indexed_words.items():
                    embed_vector = embed_dict.get(word)
                    if embed_vector is not None: 
                        embed_matrix[i] = embed_vector
              
               embed_layer = Embedding(input_dim = max_features,
                                          outputdim = 100,
                                          mask_zero = True,
                                          trainable = False,
                                          input_length = max_len,
                                          weights=[embed_matrix])
               
               model = Sequential()
               model.add(embed_layer)
               model.add(Dropout(0.25))
               model.add(LSTM(64, return_sequences = True))
               model.add(Dropout(0.25))
               model.add(LSTM(64))
               model.add(Dropout(0.25))
               model.add(Dense(label_len,activation = 'sigmoid'))


               #------------------------------------------------------------------#
               print(model.summary())

               rmsopt = optimizers.RMSprop(lr = 0.01 , decay = 0.0001)
               model.compile(optimizer = rmsopt ,loss = 'binary_crossentropy',metrics = ['accuracy'])


               model.fit(x_train,y_train, epochs = 100 , batch_size= batch_size, validation_split = 0.2)

               model.save('model_weights.h5')

               pred  = np.array(model.predict(x_test))

               predictions = np.zeros(pred.shape)
               predictions[pred > 0.5] = 1
               predictions = np.array(predictions)

               joblib.dump(ylist,'ylist')

def model_

plot = [' killer mystery sober thief' , 'comedy light hearted thief' , 'comedy thief goofup warmth childhood childhood']
x = np.array(plot)

print(x)
tk = Tokenizer(num_words=None, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True, split=" ")
tk.fit_on_texts(x)
x_train = tk.texts_to_sequences(x)

print('vector sequence')

print(x_train)

print("word to be indexed")

print(tk.word_index)

MAX_SEQUENCE_LENGTH = 6
VOCAB_SIZE = 1000
int_vectorize_layer = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode='int',
    output_sequence_length=None)

int_vectorize_layer.adapt(x)
int_vectorize_layer.get_vocabulary()
x_text_train = int_vectorize_layer(x)
print(x_text_train)
print(x_text_train.numpy())

print("word to be indexed")

print(int_vectorize_layer.get_vocabulary()[2:])
vocab_list = int_vectorize_layer.get_vocabulary()[2:]
vocab = { v: i+2 for i , v in enumerate(vocab_list)}
print(vocab)

plot = [' killer mystery sober thief' , 'comedy light hearted thief' , 'comedy thief goofup warmth childhood']